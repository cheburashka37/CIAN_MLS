{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "from implicit.lmf import LogisticMatrixFactorization\n",
    "from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,\n",
    "                                         TFIDFRecommender, bm25_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .master('yarn') \\\n",
    "       .enableHiveSupport() \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sopr = spark.read.table('prod.mles_sopr')\n",
    "#df_sopr.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_sopr = df_sopr \\\n",
    "            .select(\"user_id\", \"offer_id\", 'page_type', 'event_type') \\\n",
    "            .dropDuplicates() \\\n",
    "            .rdd\n",
    "#rdd_sopr = rf_sopr_s.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_W_FOR_PAGE_TYPE = {\"Card\" : 3,\n",
    "                        \"CardJK\" : 2,\n",
    "                        \"Listing\" : 1,\n",
    "                        \"ListingFavorites\" : 5}\n",
    "\n",
    "DICT_W_FOR_EVENT_TYPE = {\"card_show\" : 3,\n",
    "                        \"phone_show\" : 10}\n",
    "\n",
    "\n",
    "data = [Row(page_type='Card', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='CardJK', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='Listing', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='ListingFavorites', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='Card', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='CardJK', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='Listing', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='ListingFavorites', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"])] \n",
    "\n",
    "dfdict = spark.createDataFrame(sc.parallelize(data))\n",
    "dfdict.createOrReplaceTempView(\"dfdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"\"\"select distinct a.offer_num, a.offer_id, b.user_num, b.user_id, c.timestamp, c.value\n",
    "                    from (select row_number() OVER (ORDER BY a.offer_id) as offer_num, a.offer_id \n",
    "                            from (select distinct offer_id from prod.mles_sopr) as a) as a,\n",
    "                         (select row_number() OVER (ORDER BY a.user_id) as user_num, a.user_id \n",
    "                            from (select distinct user_id from prod.mles_sopr) as a) as b,\n",
    "                         (select user_id, timestamp, offer_id, value\n",
    "                            from dfdict as a, prod.mles_sopr as b \n",
    "                            where a.page_type = b.page_type and a.event_type = b.event_type) as c\n",
    "                         where c.offer_id = a.offer_id \n",
    "                               and c.user_id = b.user_id\n",
    "                               and b.user_id != 'noid'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "DICT_W_FOR_PAGE_TYPE = {\"Card\" : 3,\n",
    "                        \"CardJK\" : 2,\n",
    "                        \"Listing\" : 1,\n",
    "                        \"ListingFavorites\" : 5}\n",
    "\n",
    "DICT_W_FOR_EVENT_TYPE = {\"card_show\" : 3,\n",
    "                        \"phone_show\" : 10}\n",
    "\n",
    "#разделение на 9 частей по времени\n",
    "def lambdaForArr(x):\n",
    "    return (x['user_id'], [x['offer_id'], \n",
    "                           DICT_W_FOR_PAGE_TYPE[x['page_type']] * DICT_W_FOR_EVENT_TYPE[x['event_type']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = sqlDF.collect()\n",
    "          #.randomSplit([1, 500])[0].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[1]['offer_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_mtrx(arr):\n",
    "    with tqdm.tqdm(total=len(arr)) as progress:\n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "\n",
    "        for i in arr:\n",
    "            row.append(i['user_num'])\n",
    "            col.append(i['offer_num'])\n",
    "            data.append(i['value'])\n",
    "            progress.update(1)\n",
    "\n",
    "        #data = np.ones(len(indices))\n",
    "        #mtrx = csr_matrix((data, indices, indptr))\n",
    "    return csr_matrix((data, (row, col)))#.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrx = get_mtrx(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlternatingLeastSquares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(mtrx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
