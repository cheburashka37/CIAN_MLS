{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "from scipy.sparse import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .master('yarn') \\\n",
    "       .enableHiveSupport() \\\n",
    "       .getOrCreate()\n",
    "\n",
    "#.master('yarn') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_W_FOR_PAGE_TYPE = {\"Card\" : 3,\n",
    "                        \"CardJK\" : 2,\n",
    "                        \"Listing\" : 1,\n",
    "                        \"ListingFavorites\" : 5}\n",
    "\n",
    "DICT_W_FOR_EVENT_TYPE = {\"card_show\" : 3,\n",
    "                        \"phone_show\" : 10}\n",
    "\n",
    "data=[('Card', 'card_show', int(DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"])), \n",
    "        ('CardJK', 'card_show', int(DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"])), \n",
    "        ('Listing', 'card_show', int(DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"])), \n",
    "        ('ListingFavorites', 'card_show', int(DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"])), \n",
    "        ('Card', 'phone_show', int(DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"])), \n",
    "        ('CardJK', 'phone_show', int(DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"])), \n",
    "        ('Listing', 'phone_show', int(DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"])), \n",
    "        ('ListingFavorites', 'phone_show', int(DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]))]\n",
    "\n",
    "#spark.createDataFrame(data, ['page_type', 'event_type', 'value']).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfdict = spark.createDataFrame(data, ['page_type', 'event_type', 'value'])\n",
    "dfdict.createOrReplaceTempView(\"dfdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareRdd(x,y):\n",
    "    if (x['offer_num'] > y['offer_num']):\n",
    "        return x\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVector(x,y):\n",
    "    if type(x) is list:\n",
    "        if type(y) is list:\n",
    "            return x + y\n",
    "        elif type(y) is tuple:\n",
    "            return x + [y]\n",
    "        else: \n",
    "            raise BaseException('Wrong type of y')\n",
    "    elif type(x) is tuple:\n",
    "        if type(y) is list:\n",
    "            return [x] + y\n",
    "        elif type(y) is tuple:\n",
    "            return [x] + [y]\n",
    "        else: \n",
    "            raise BaseException('Wrong type of y')\n",
    "    else:\n",
    "        raise BaseException('Wrong type of x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortVector(a):\n",
    "    if type(a[1]) is list:\n",
    "        a[1].sort(key=lambda t: t[0])\n",
    "        #print(a)\n",
    "        b = []\n",
    "        c = []\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            #print(i)\n",
    "            if a[1][i][0] == a[1][i+1][0]:\n",
    "                if a[1][i][1] < a[1][i+1][1]:\n",
    "                    t = a[1].pop(i)\n",
    "                else:\n",
    "                    t = a[1].pop(i+1)\n",
    "                #print('pop', t)\n",
    "\n",
    "            else:\n",
    "                i = i + 1\n",
    "                b.append(a[1][i][0])\n",
    "                c.append(a[1][i][1])\n",
    "                #print('add', (a[1][i][0], a[1][i][1]))\n",
    "            if i == len(a[1]) - 1:\n",
    "                break\n",
    "\n",
    "        return (a[0], (b, c))\n",
    "    elif type(a[1]) is tuple:\n",
    "        return (a[0], ([a[1][0]], [a[1][1]]))\n",
    "    else:\n",
    "        raise BaseException('Wrong type of a: ' + str(type(a[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sql(\"\"\"select distinct a.offer_num, a.offer_id, b.user_num, b.user_id, c.value\n",
    "                    from (select user_id, offer_id, value\n",
    "                            from dfdict as a, prod.mles_sopr as b \n",
    "                            where a.page_type = b.page_type and a.event_type = b.event_type) as c\n",
    "                         INNER JOIN (select row_number() OVER (ORDER BY a.user_id) as user_num, a.user_id \n",
    "                            from (select distinct user_id from prod.mles_sopr limit 10000) as a) as b on c.user_id = b.user_id\n",
    "                         LEFT JOIN (select row_number() OVER (ORDER BY a.offer_id) as offer_num, a.offer_id \n",
    "                            from (select distinct offer_id from prod.mles_sopr) as a) as a on c.offer_id = a.offer_id \n",
    "                         where b.user_id != 'noid'\"\"\") \\\n",
    "              .rdd.repartition(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rdd = sqlDF.rdd.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ca0627dd5928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaxOfferNum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompareRdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'offer_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rdd' is not defined"
     ]
    }
   ],
   "source": [
    "maxOfferNum = rdd.reduce(lambda x, y: compareRdd(x,y))['offer_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = rdd.map(lambda x: (x['user_num'], (x['offer_num'], x['value']))) \\\n",
    "            .reduceByKey(lambda x,y: createVector(x, y)) \\\n",
    "            .map(lambda x: sortVector(x)) \\\n",
    "            .map(lambda x: Vectors.sparse(maxOfferNum + 1, x[1][0], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetSize = target.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = target.randomSplit([(targetSize - 500)/targetSize, 500/targetSize])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_K = 0\n",
    "STEP_K = 100\n",
    "cost = np.zeros(10)\n",
    "\n",
    "with tqdm.tqdm(total=10) as progress:\n",
    "    for i in range(10):\n",
    "        model = KMeans.train(X,\n",
    "                     i*10 + 1, \n",
    "                     initializationMode=\"k-means||\", \n",
    "                     initializationSteps=5, \n",
    "                     epsilon=1e-4)\n",
    "        \n",
    "        cost[i] = model.computeCost(X)\n",
    "        print(i*10 + 1, cost[i])\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,10))\n",
    "plt.plot(np.arange(10) * 10 + 1, cost)\n",
    "\n",
    "plt.title('cost from num_klasters')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('num_klasters')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
