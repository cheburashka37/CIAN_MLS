{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "MONTHTEST = 3\n",
    "DAY = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .master('yarn') \\\n",
    "       .enableHiveSupport() \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_W_FOR_PAGE_TYPE = {\"Card\" : 3,\n",
    "                        \"CardJK\" : 2,\n",
    "                        \"Listing\" : 1,\n",
    "                        \"ListingFavorites\" : 5}\n",
    "\n",
    "DICT_W_FOR_EVENT_TYPE = {\"card_show\" : 3,\n",
    "                        \"phone_show\" : 10}\n",
    "\n",
    "\n",
    "data = [Row(page_type='Card', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='CardJK', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='Listing', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='ListingFavorites', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='Card', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='CardJK', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='Listing', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='ListingFavorites', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"])] \n",
    "\n",
    "dfdict = spark.createDataFrame(sc.parallelize(data))\n",
    "dfdict.createOrReplaceTempView(\"dfdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item = spark.sql(\"\"\"select a.user_id, a.offer_id from \n",
    "                            (select user_id, offer_id\n",
    "                                from prod.mles_sopr\n",
    "                                where ptn_dadd between '2019-06-01' and '2019-06-21') as a\n",
    "                        inner join \n",
    "                            (select user_id, offer_id\n",
    "                                from prod.mles_sopr\n",
    "                                where ptn_dadd between '2019-06-22' and '2019-07-03') as b\n",
    "                        on a.user_id = b.user_id \n",
    "                        where a.user_id != 'noid'\n",
    "                        limit 10000\n",
    "                  \"\"\").repartition(100).createOrReplaceTempView(\"user_item\")\n",
    "\n",
    "\n",
    "\n",
    "#df_sopr.createOrReplaceTempView(\"df_sopr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"\"\"select distinct a.offer_num, a.offer_id, b.user_num, b.user_id, c.value, ptn_dadd\n",
    "                    from (select user_id, offer_id, value, ptn_dadd\n",
    "                            from dfdict as a, prod.mles_sopr as b \n",
    "                            where a.page_type = b.page_type and a.event_type = b.event_type) as c\n",
    "                         INNER JOIN (select row_number() OVER (ORDER BY a.user_id) as user_num, a.user_id \n",
    "                            from (select distinct user_id from user_item) as a) as b on c.user_id = b.user_id\n",
    "                         INNER JOIN (select row_number() OVER (ORDER BY a.offer_id) as offer_num, a.offer_id \n",
    "                            from (select distinct offer_id from user_item) as a) as a on c.offer_id = a.offer_id \n",
    "                  \"\"\").repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlDFtrain = sqlDF.where(\"ptn_dadd between '2019-06-01' and '2019-06-21'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDFtest = sqlDF.where(\"ptn_dadd between '2019-06-22' and '2019-07-03'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_num\", itemCol=\"offer_num\", ratingCol=\"value\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(sqlDFtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 10 movie recommendations for each user\n",
    "k = 10\n",
    "\n",
    "userRecs = model.recommendForAllUsers(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = spark.sql(\"\"\"select count(*) from (select distinct user_id from user_item)\n",
    "                  \"\"\").collect()[0]['count(1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:58<00:00, 51.68s/it]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "with tqdm.tqdm(total=20) as progress:\n",
    "    for j in range(20):\n",
    "        progress.update(1)\n",
    "        user = int(random.random() * num_users)\n",
    "        t = sqlDFtest.where(\"user_num = \" + str(user)).select(\"offer_num\").collect()\n",
    "        viewed = [i['offer_num'] for i in t]\n",
    "        if len(viewed) == 0:\n",
    "            continue\n",
    "\n",
    "        t = userRecs.where('user_num = ' + str(user)).collect()\n",
    "        if len(t) == 0:\n",
    "            continue\n",
    "\n",
    "        t = t[0]['recommendations']\n",
    "        recom = [i['offer_num'] for i in t]\n",
    "\n",
    "\n",
    "        sum += len(np.intersect1d(recom, viewed))/k\n",
    "        #print(\"    \", user, \" sum =\", sum)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
