{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "from scipy.sparse import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "import random\n",
    "\n",
    "NUMCLASTERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .master('yarn') \\\n",
    "       .enableHiveSupport() \\\n",
    "       .getOrCreate()\n",
    "\n",
    "#.master('yarn') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst = spark.read.table('prod.announcement_lst').repartition(100)\n",
    "#df_lst.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy import array\n",
    "DICT = {'bedRent': 1,\n",
    "        'commercialLandRent': 2,\n",
    "        'dailyHouseRent': 3,\n",
    "        'flatRent': 4,\n",
    "        'flatSale': 5,\n",
    "        'freeAppointmentObjectRent': 6,\n",
    "        'officeRent': 7,\n",
    "        'roomRent': 8,\n",
    "        'shoppingAreaRent': 9,\n",
    "        'warehouseSale': 10,\n",
    "        'cottageSale': 11,\n",
    "        'flatShareSale': 12,\n",
    "        'freeAppointmentObjectSale': 13,\n",
    "        'garageRent': 14,\n",
    "        'garageSale': 15,\n",
    "        'industryRent': 16,\n",
    "        'newBuildingFlatSale': 17,\n",
    "        'townhouseRent': 18,\n",
    "        'buildingSale': 19,\n",
    "        'cottageRent': 20,\n",
    "        'dailyFlatRent': 21,\n",
    "        'houseRent': 22,\n",
    "        'houseSale': 23,\n",
    "        'landSale': 24,\n",
    "        'roomSale': 25,\n",
    "        'warehouseRent': 26,\n",
    "        'buildingRent': 27,\n",
    "        'shoppingAreaSale': 28,\n",
    "        'townhouseSale': 29,\n",
    "        'businessRent': 30,\n",
    "        'businessSale': 31,\n",
    "        'dailyBedRent': 32,\n",
    "        'dailyRoomRent': 33,\n",
    "        'houseShareRent': 34,\n",
    "        'houseShareSale': 35,\n",
    "        'industrySale': 36,\n",
    "        'officeSale': 37,\n",
    "        'commercialLandSale': 38}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda1(x):\n",
    "    if x['priceType'] == 'squareMeter':\n",
    "        if (x['price'] is None) or (x['totalarea'] is None):\n",
    "            totalprice = 0\n",
    "        else:\n",
    "            totalprice = x['price'] * x['totalarea']\n",
    "    elif x['price'] is None:\n",
    "        totalprice = 0\n",
    "    else: \n",
    "        totalprice = x['price']\n",
    "    \n",
    "    if x['floornumber'] is None:\n",
    "        floornumber = 0\n",
    "    else:\n",
    "        floornumber = x['floornumber']\n",
    "    \n",
    "    if x['floorscount'] is None:\n",
    "        floorscount = 0\n",
    "    else:\n",
    "        floorscount = x['floorscount']\n",
    "    \n",
    "    if x['category'] is None:\n",
    "        category = 0\n",
    "    else:\n",
    "        category = DICT[x['category']] * 2\n",
    "    \n",
    "    if x['roomscount'] is None:\n",
    "        roomscount = 0\n",
    "    else:\n",
    "        roomscount = x['roomscount']\n",
    "    \n",
    "    if x['totalarea'] is None:\n",
    "        totalarea = 0\n",
    "    else:\n",
    "        totalarea = x['totalarea']\n",
    "    \n",
    "    return np.array([x['announcementid'],\n",
    "                     floornumber, \n",
    "                     floorscount, \n",
    "                     category, \n",
    "                     roomscount, \n",
    "                     totalarea, \n",
    "                     totalprice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddSoprWithId = df_lst.select('announcementid', \n",
    "                         'floornumber', \n",
    "                         'floorscount', \n",
    "                         'category', \n",
    "                         'roomscount', \n",
    "                         'totalarea', \n",
    "                         'price',\n",
    "                         'priceType') \\\n",
    "                 .dropDuplicates() \\\n",
    "                 .rdd \\\n",
    "                 .map(lambda x: lambda1(x)) #.show()\n",
    "X = rddSoprWithId.map(lambda x: x[1:]).randomSplit([(2918553 - 1000)/2918553, 1000/2918553])[1]\n",
    "#rdd_lst = df_lst.limit(10000).rdd#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans.train(X,\n",
    "                     NUMCLASTERS, \n",
    "                     initializationMode=\"k-means||\", \n",
    "                     initializationSteps=5, \n",
    "                     epsilon=1e-4)\n",
    "\n",
    "#model.save(\"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addpredict(x):\n",
    "    p = model.predict(x[1:])\n",
    "    return (int(x[0]), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddSoprWithIdPred = rddSoprWithId.map(lambda x: addpredict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_W_FOR_PAGE_TYPE = {\"Card\" : 3,\n",
    "                        \"CardJK\" : 2,\n",
    "                        \"Listing\" : 1,\n",
    "                        \"ListingFavorites\" : 5}\n",
    "\n",
    "DICT_W_FOR_EVENT_TYPE = {\"card_show\" : 3,\n",
    "                        \"phone_show\" : 10}\n",
    "\n",
    "\n",
    "data = [Row(page_type='Card', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='CardJK', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='Listing', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='ListingFavorites', event_type='card_show', value = DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"card_show\"]), \n",
    "        Row(page_type='Card', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"Card\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='CardJK', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"CardJK\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='Listing', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"Listing\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"]), \n",
    "        Row(page_type='ListingFavorites', event_type='phone_show', value = DICT_W_FOR_PAGE_TYPE[\"ListingFavorites\"] * DICT_W_FOR_EVENT_TYPE[\"phone_show\"])] \n",
    "\n",
    "dfdict = spark.createDataFrame(sc.parallelize(data))\n",
    "dfdict.createOrReplaceTempView(\"dfdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item = spark.sql(\"\"\"select a.user_id, a.offer_id from \n",
    "                            (select user_id, offer_id\n",
    "                                from prod.mles_sopr\n",
    "                                where ptn_dadd between '2019-06-01' and '2019-06-21') as a\n",
    "                        inner join \n",
    "                            (select user_id, offer_id\n",
    "                                from prod.mles_sopr\n",
    "                                where ptn_dadd between '2019-06-22' and '2019-07-03') as b\n",
    "                        on a.user_id = b.user_id \n",
    "                        where a.user_id != 'noid'\n",
    "                        limit 10000\n",
    "                  \"\"\").repartition(100).createOrReplaceTempView(\"user_item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"\"\"select distinct a.offer_num, a.offer_id, b.user_num, b.user_id, c.value, ptn_dadd\n",
    "                    from (select user_id, offer_id, value, ptn_dadd\n",
    "                            from dfdict as a, prod.mles_sopr as b \n",
    "                            where a.page_type = b.page_type and a.event_type = b.event_type) as c\n",
    "                         INNER JOIN (select row_number() OVER (ORDER BY a.user_id) as user_num, a.user_id \n",
    "                            from (select distinct user_id from user_item) as a) as b on c.user_id = b.user_id\n",
    "                         INNER JOIN (select row_number() OVER (ORDER BY a.offer_id) as offer_num, a.offer_id \n",
    "                            from (select distinct offer_id from user_item) as a) as a on c.offer_id = a.offer_id \n",
    "                  \"\"\").repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDFtrain = sqlDF.where(\"ptn_dadd between '2019-06-01' and '2019-06-21'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDFtest = sqlDF.where(\"ptn_dadd between '2019-06-22' and '2019-07-03'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddtrain = sqlDFtrain.rdd.map(lambda x: (x['offer_id'], x)).join(rddSoprWithIdPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "listrdd = []\n",
    "for i in range(NUMCLASTERS):\n",
    "    test = rddtrain.filter(lambda x: x[1][1] == i)\n",
    "    if test.isEmpty():\n",
    "        listrdd.append(None)\n",
    "        continue\n",
    "    else:\n",
    "        listrdd.append(spark.createDataFrame(test.map(lambda x: x[1][0])))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoolALS(x):\n",
    "    als = ALS(maxIter=5, regParam=0.01, userCol=\"user_num\", itemCol=\"offer_num\", ratingCol=\"value\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "    model = als.fit(x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:09<00:00, 12.34s/it]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "modelList = []\n",
    "recsList = []\n",
    "with tqdm.tqdm(total=NUMCLASTERS) as progress:\n",
    "    for rdds in listrdd:\n",
    "        if rdds:\n",
    "            model = PoolALS(rdds)\n",
    "            modelList.append(model)\n",
    "            user_recs = model.recommendForAllUsers(k)\n",
    "            recsList.append(user_recs)\n",
    "        else:\n",
    "            modelList.append(None)\n",
    "            recsList.append(None)\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = spark.sql(\"\"\"select count(*) from (select distinct user_id from user_item)\n",
    "                  \"\"\").collect()[0]['count(1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [06:48<00:23, 23.73s/it]"
     ]
    }
   ],
   "source": [
    "sum = [0] * NUMCLASTERS\n",
    "with tqdm.tqdm(total=20) as progress:\n",
    "    for j in range(20):\n",
    "        progress.update(1)\n",
    "        user = int(random.random() * num_users)\n",
    "        t = sqlDFtest.where(\"user_num = \" + str(user)).select(\"offer_num\").collect()\n",
    "        viewed = [i['offer_num'] for i in t]\n",
    "        if len(viewed) == 0:\n",
    "            continue\n",
    "        \n",
    "        for l in range(NUMCLASTERS):\n",
    "            recs = recsList[l]\n",
    "            if recs:\n",
    "                t = recs.where('user_num = ' + str(user)).collect()\n",
    "                if len(t) == 0:\n",
    "                    continue\n",
    "\n",
    "                t = t[0]['recommendations']\n",
    "                recom = [i['offer_num'] for i in t]\n",
    "\n",
    "\n",
    "                sum[l] += len(np.intersect1d(recom, viewed))/k\n",
    "        #print(\"    \", user, \" sum =\", sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
